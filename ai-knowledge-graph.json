[
  {
    "id": "agi_artificial_general_intelligence",
    "term": "AGI (Artificial General Intelligence)",
    "definition": "AI that can do any intellectual task a human can.",
    "explanation": "While some define AGI as AI that’s as smart as a human in every way, this isn’t something you need to focus on right now. It’s more important to build AI solutions that solve your specific problems today.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "ai_aritificial_intelligence"
      }
    ]
  },
  {
    "id": "ai_aritificial_intelligence",
    "term": "AI (Aritificial Intelligence)",
    "definition": "A language model like ChatGPT",
    "explanation": "Colloquially we say AI to refer to the current generation LLMs / models. The field of AI is broad and spans decades, covers Machine Learning, Computer Vision, Speech recognition, AGI, Robotics, etc.",
    "category": "Basics",
    "edges": [
      {
        "type": "synonym",
        "target": "model"
      }
    ]
  },
  {
    "id": "model_characteristics",
    "term": "Model Characteristics",
    "definition": "Characteristics of an AI that are observed during usage, that aren't necessarily explained easily by theory",
    "explanation": "Neural nets have long been black boxes, so knowing the general characteristics of the model helps in using it carefully, and effectively.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "hallucination"
      },
      {
        "type": "related",
        "target": "stochastic"
      },
      {
        "type": "related",
        "target": "model"
      }
    ]
  },
  {
    "id": "assistant___instruct_model",
    "term": "Assistant / Instruct model",
    "definition": "A base model that has been fine-tuned to be a honest, harmless, and helpful assistant.",
    "explanation": "This is the type of model most people interact with. The fine-tuning process is what makes it useful for real-world tasks.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "fine_tuning"
      }
    ]
  },
  {
    "id": "base_model",
    "term": "Base model",
    "definition": "A powerful text-completion engine but not a conversational assistant.",
    "explanation": "There are only 10s of these out there because they're prohibitively expensive to train. They are foundational building blocks that can be customised, and taught new skills or behaviours.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "model"
      }
    ]
  },
  {
    "id": "context",
    "term": "Context",
    "definition": "The input or question you give to the AI",
    "explanation": "Giving clear and detailed instructions helps the AI understand what you want. Just like talking to a person, good communication gets better results. The context is the model's \"short-term memory\" and it limits the complexity of tasks it can perform.",
    "category": "Basics",
    "edges": [
      {
        "type": "synonym",
        "target": "prompt"
      },
      {
        "type": "related",
        "target": "system_prompt"
      },
      {
        "type": "related",
        "target": "inference"
      }
    ],
    "synonyms": "Prompt"
  },
  {
    "id": "hallucination",
    "term": "Hallucination",
    "definition": "When AI makes up things that aren’t true.",
    "explanation": "AIs sometimes make stuff up, and you can’t completely stop this. It’s important to be aware that mistakes can happen, so you should check the AI’s answers.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "inference",
    "term": "Inference",
    "definition": "Getting an answer back from the model.",
    "explanation": "When you ask the AI a question and it gives you an answer, that’s called inference. It’s the process of the AI making predictions or responses. Knowing this helps you understand how the AI works and the time or resources it might need to give answers.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "model"
      }
    ]
  },
  {
    "id": "neural_networks",
    "term": "Neural Networks",
    "definition": "Computational models inspired by the brain's structure",
    "explanation": "They are the core technology that enables AI to learn from text and generate human-like responses.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "prompt",
    "term": "Prompt",
    "definition": "The input or question you give to the AI.",
    "explanation": "Giving clear and detailed prompts helps the AI understand what you want. Just like talking to a person, good communication gets better results.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "stochastic",
    "term": "Stochastic",
    "definition": "AI's Nature of being probabalistic, rolling the dice",
    "explanation": "This is why you can get a different answer every time you ask the same question. It allows for creativity but also unpredictability.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "system_prompt",
    "term": "System Prompt",
    "definition": "A hidden instruction given to the model about its persona, rules, and identity.",
    "explanation": "It's a powerful way to steer the AI's behavior consistently without the user seeing the underlying instructions.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "tokens",
    "term": "Tokens",
    "definition": "The pieces of text that a model understands, it can be a word, a part of a word, or a character.",
    "explanation": "Token impacts model abilities, especially with tasks like spelling or character manipulation. Also, you pay for AI based on the number of tokens used, so knowing about tokens helps manage costs. The vocabulary of tokens depends on training data, and it impacts performance in multilingual or esoteric domains.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "tokenisation"
      },
      {
        "type": "related",
        "target": "encoding"
      },
      {
        "type": "related",
        "target": "vocabulary"
      },
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "tokenisation",
    "term": "tokenisation"
  },
  {
    "id": "encoding",
    "term": "Encoding",
    "definition": "The process of converting data from a human-readable format (e.g., text, images, audio) into a numerical representation that a machine learning model can process.",
    "explanation": "AI models operate on numbers, not raw data like pixels or characters. Encoding is the crucial bridge that translates real-world information into a mathematical format the model can process, making it the foundational first step for any AI task.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "tokenisation"
      }
    ]
  },
  {
    "id": "distillation",
    "term": "Distillation",
    "definition": "Making a smaller, faster model from a big one.",
    "explanation": "It lets you use cheaper, faster models with less delay (latency). But the smaller model might not be as accurate or powerful as the big one. So, you trade some performance for speed and cost savings.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "hyperparameters",
    "term": "Hyperparameters",
    "definition": "Settings that affect how the model works.",
    "explanation": "These include the number of layers, vocabulary size, and dimensions of the model. By adjusting these settings, you can make the AI learn better. It often takes trying different options to find what works best.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "knowledge_cut_off",
    "term": "Knowledge cut-off",
    "definition": "The date beyond which a model has no information.",
    "explanation": "This is a fundamental limitation of current LLMs, because they stop learning after training is complete. They don't have real-time knowledge unless they are augmented with tools like web search.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "loss",
    "term": "Loss",
    "definition": "A number that measures how wrong the model's predictions are during training.",
    "explanation": "The goal of training is to minimize the loss. Watching the loss go down is how researchers know the model is learning.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "multimodal",
    "term": "Multimodal",
    "definition": "Models that handle different data types like text and images.",
    "explanation": "People use words, pictures, and sounds. When AI can understand all these, it can help users better. Using multimodal AI makes your tools more powerful.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "neural_network_architecture",
    "term": "Neural network architecture",
    "definition": "Design of the math functions that create the neural network",
    "explanation": "It influences ability to learn, scalability, modalities, and training / inference cost. Breakthroughs here are big. Examples include Transformer, Mixture of Experts (MoE), State Space Models (SSMs), Multimodal / Unified, Matryoshka Representation, etc.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "neural_networks"
      },
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "parameters",
    "term": "Parameters",
    "definition": "The learned \"long-term\" memory, represented as billions of numbers",
    "explanation": "Providing more parameters allows training on more data and creating more capabilities. It's also slower and more expensive to train and run. Conversely, smaller models are cheaper and faster, but lesser capable.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "transformer",
    "term": "Transformer",
    "definition": "The main type of neural network architecture used in AI today",
    "explanation": "It provides the crucial ability for understanding language, letting the model focus on relevant parts of input. It makes language models faster, more accurate and capable of capturing nuanced meaning.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "attention"
      },
      {
        "type": "related",
        "target": "neural_network_architecture"
      }
    ]
  },
  {
    "id": "attention",
    "term": "Attention",
    "definition": "Mechanism that lets the model focus on relevant parts of the input",
    "explanation": "This is a crucial ability for understanding language. It makes language models faster, more accurate and capable of capturing nuanced meaning.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "vocabulary",
    "term": "Vocabulary",
    "definition": "The complete set of unique tokens that a model can recognize and produce.",
    "explanation": "The vocabulary defines the atomic units of language the model works with. Its size is a key design choice in an LLM's architecture.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "chain_of_thought",
    "term": "Chain of Thought",
    "definition": "Prompting the model to think and plan before answering.",
    "explanation": "When the model thinks first, it gives better answers but takes longer. This trade-off affects speed and quality.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "fine_tuning",
    "term": "Supervised Fine-Tuning",
    "definition": "Adjusting a pre-trained model for a specific job.",
    "explanation": "It helps make the AI better for your needs by teaching it with your data. But it might become less good at general tasks. Fine-tuning works best for specific jobs where you need higher accuracy.",
    "category": "Post training",
    "edges": []
  },
  {
    "id": "tools",
    "term": "Tools",
    "definition": "Giving AI the ability to call external programs, like a web search, a calculator, a database, or a code interpreter.",
    "explanation": "This overcomes the model's inherent limitations (like knowledge cut-offs or poor arithmetic) by allowing it to access real-time data and precise computation. Having tools to access the file system or databases can support the \"skill\" of programming.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "guardrails",
    "term": "Guardrails",
    "definition": "Safety rules to control model outputs.",
    "explanation": "Guardrails help reduce the chance of the AI giving bad or harmful answers. But they are not perfect. It’s important to use them wisely and not rely on them completely.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "thinking_model",
    "term": "Thinking model",
    "definition": "A model that has undergone extensive RL, making it adept at complex, multi-step problem solving.",
    "explanation": "These models represent the frontier of AI capabilities, moving from simple Q&A to genuine problem-solving.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "reinforcement_learning_rl",
    "term": "Reinforcement Learning (RL)",
    "definition": "Training by making the AI do exercises and learn from its mistakes",
    "explanation": "RL allows the model to go beyond imitating human experts and discover novel, more effective strategies for solving problems on its own. This is how it acquired difficult skills like solving math problems, or writing code.\n\nIt can get the feedback from humans (RLHF) or from another AI model (RLAIF).",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "rlhf"
      },
      {
        "type": "related",
        "target": "rlaif"
      }
    ]
  },
  {
    "id": "rlhf",
    "term": "Reinforcement Learning from Human Feedback (RLHF)",
    "definition": "A technique that uses human preferences to train a \"reward model\" which then guides the reinforcement learning process.",
    "explanation": "RLHF is the key that unlocks RL for subjective, unverifiable domains like writing quality or helpfulness. It's used to fine-tune most modern chatbots.",
    "category": "General",
    "edges": []
  },
  {
    "id": "rlaif",
    "term": "RLAIF"
  },
  {
    "id": "agents",
    "term": "Agents",
    "definition": "AI systems that can autonomously perform a sequence of tasks over time to achieve a complex goal.",
    "explanation": "Agents represent the shift from single-shot commands to having AI assistants that can manage long-running jobs and projects.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "ai_engineering"
      }
    ]
  },
  {
    "id": "open_weights_model",
    "term": "Open weights model",
    "definition": "A model whose parameters are publicly released, allowing anyone to use, modify, and build upon it.",
    "explanation": "This fosters innovation and competition, preventing the most powerful AI technology from being controlled by only a handful of large corporations.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "parameters"
      }
    ]
  },
  {
    "id": "context_engineering",
    "term": "Context Engineering",
    "definition": "Context engineering is the art and science of strategically assembling all necessary information—such as task descriptions, few-shot examples, retrieved data (RAG), tools, and conversation history—into an LLM's context window to optimize its performance for the next step.",
    "explanation": "An LLM's performance is critically dependent on the quality and relevance of the information in its context window. Providing too little information leads to poor results, while too much or irrelevant information increases costs and can degrade performance.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "ai_engineering"
      }
    ]
  },
  {
    "id": "embeddings",
    "term": "Embeddings",
    "definition": "Turning words into numbers that show meaning.",
    "explanation": "Embeddings let you search documents by meaning, not just exact words. This helps you find information even if different words are used, making searches smarter and more accurate.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "vector_database"
      }
    ]
  },
  {
    "id": "few_shot_learning",
    "term": "Few-Shot Learning",
    "definition": "Teaching the model with only a few examples.",
    "explanation": "By giving the model examples, you can guide it to behave the way you want. It’s a simple but powerful way to teach the AI what is good or bad.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "retrieval_augmented_generation_rag",
    "term": "Retrieval Augmented Generation (RAG)",
    "definition": "Providing relevant context to the LLM.",
    "explanation": "A language model needs proper context to answer questions. Like a person, it needs access to information such as data, past conversations, or documents to give a good answer. Collecting and giving this info to the AI before asking helps prevent mistakes or it saying, “I don’t know.”",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "vector_database"
      },
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "temperature",
    "term": "Temperature",
    "definition": "A setting that controls how creative AI responses are.",
    "explanation": "Lets you choose between predictable or more imaginative answers. Adjusting temperature can affect the quality and usefulness of the AI’s responses.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "top_p_sampling",
    "term": "Top-p Sampling",
    "definition": "Choosing the next word from top choices making up a set probability.",
    "explanation": "Balances predictability and creativity in AI responses. The trade-off is between safe answers and more varied ones.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "vector_database",
    "term": "Vector Database",
    "definition": "A special database for storing and searching embeddings.",
    "explanation": "They store embeddings of text, images, and more, so you can search by meaning. This makes finding similar items faster and improves searches and recommendations.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "t",
    "term": "Pre-Training",
    "definition": "The initial stage of building an LLM, where a neural network learns from a vast dataset of text from the internet.",
    "explanation": "This is where the model gains its foundational knowledge about language, facts, and reasoning. The massive scale of this stage is what gives LLMs their broad capabilities.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "post_training",
    "term": "Post-training",
    "definition": "The second phase of LLM creation, where a pre-trained base model is adapted to become a helpful, instruction-following assistant.",
    "explanation": "This stage bridges the gap between a raw \"internet simulator\" and a useful tool like ChatGPT, aligning the model with human intent.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      },
      {
        "type": "related",
        "target": "fine_tuning"
      },
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "prompt_injection",
    "term": "Prompt Injection",
    "definition": "A security risk where bad instructions are added to prompts.",
    "explanation": "Users might try to trick the AI into ignoring your rules and doing things you don’t want. Knowing about prompt injection helps you protect your AI system from misuse.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "prompt_templates",
    "term": "Prompt Templates",
    "definition": "Pre-made formats for prompts to keep inputs consistent.",
    "explanation": "They help you communicate with the AI consistently by filling in blanks in a set format. This makes it easier to use the AI in different situations and ensures you get good results.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "sampling",
    "term": "Sampling",
    "definition": "Settings that control predictability, and creativity.",
    "explanation": "Higher temperature for more creative responses, and lower top-p / top-k for more predictable responses. Generating fewer responses also save compute resources.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "temperature"
      },
      {
        "type": "related",
        "target": "top_p_sampling"
      },
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "ai_engineering",
    "term": "AI Engineering",
    "definition": "Using AI effectively to create products for end users",
    "explanation": "TODO",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "diffusion",
    "term": "Diffusion",
    "definition": "Diffusion in AI is a method that generates images, audio or video by gradually improving random noise.",
    "explanation": "Diffusion often uses the U-Net architecture, which is different from Transformers. Understanding diffusion helps demystify how modern AI art and media tools actually work.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "neural_network_architecture"
      }
    ]
  },
  {
    "id": "quantisation",
    "term": "Quantisation",
    "definition": "Quantisation is a technique that makes AI models smaller and faster by using lower precision numbers.",
    "explanation": "By using fewer bits to represent numbers, quantisation allows AI to run on phones, smart devices, or in low-power environments without needing powerful servers",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "evals",
    "term": "Evals",
    "definition": "Evals are ways to check if an AI is giving useful, accurate, and safe answers.",
    "explanation": "Evals are how you turn an AI demo into a reliable product. They guide improvements, catch bugs before users do, and give teams confidence that changes are actually making things better. Evals can be simple unit tests, human-evals, or LLM-as-a-judge.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "ai_engineering"
      }
    ]
  },
  {
    "id": "multi_agent_system",
    "term": "Multi Agent System",
    "definition": "A group of agents that collaborate to achieve a single goal.",
    "explanation": "This is useful to work around the context limits, parallelise tasks, and cover more ground in problems like research or decision making.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "agents"
      }
    ]
  },
  {
    "id": "traces",
    "term": "Traces",
    "definition": "Traces let you observe what an AI model does when handling a request, step by step.",
    "explanation": "Traces help you analyse behaviour, diagnose issues, and generate insights to improve performance and design.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "ai_engineering"
      }
    ]
  },
  {
    "id": "distillation_risk",
    "term": "Distillation risk",
    "definition": "The risk that a competitor could steal a model's advanced reasoning abilities by training their own model on its detailed outputs.",
    "explanation": "This is why some companies hide the \"thinking\" process of their reasoning models, as it's a valuable and hard-won capability.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "distillation"
      }
    ]
  },
  {
    "id": "human_labelers",
    "term": "Human labelers",
    "definition": "People hired to create the ideal conversational data used for SFT.",
    "explanation": "They are the source of the \"good\" behavior that the model learns to imitate. The quality of their work directly shapes the AI's personality and helpfulness.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "fine_tuning"
      }
    ]
  },
  {
    "id": "layer_norms_matrix_multiplications_softmaxes",
    "term": "Layer norms, Matrix multiplications, Softmaxes",
    "definition": "The fundamental mathematical operations that make up the layers of a Transformer network.",
    "explanation": "These simple, repeated calculations, when performed at a massive scale, enable the complex processing that LLMs do.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "llm_judge",
    "term": "LLM Judge",
    "definition": "Using a powerful LLM to automatically evaluate the quality of another LLM's output.",
    "explanation": "This is a scalable way to get feedback for training, especially in domains that are hard to score with simple rules.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "evals"
      }
    ]
  },
  {
    "id": "logits___softmax",
    "term": "Logits / Softmax",
    "definition": "The raw scores (logits) and resulting probabilities (softmax) that the model assigns to each possible next token.",
    "explanation": "This output distribution represents the model's prediction. The model then samples from these probabilities to choose the next token.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "maximum_context_length",
    "term": "Maximum context length",
    "definition": "The maximum number of tokens the model can process at one time to make a prediction.",
    "explanation": "This is a critical limitation. A longer context length allows the model to understand more complex documents and maintain longer conversations.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "context"
      }
    ]
  },
  {
    "id": "reward_model",
    "term": "Reward model",
    "definition": "A model trained to predict which of two responses a human is likely to prefer.",
    "explanation": "It acts as a scalable, automated proxy for human judgment, making it possible to apply RL to improve qualities like helpfulness and harmlessness.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "special_tokens",
    "term": "Special tokens",
    "definition": "New tokens added to the vocabulary to structure conversations, marking the beginning and end of turns for the user and assistant.",
    "explanation": "They provide the necessary structure for the model to understand the format of a dialogue, which is different from the free-form text it saw in pre-training.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "fine_tuning"
      }
    ]
  },
  {
    "id": "unverifiable_domains",
    "term": "Unverifiable domains",
    "definition": "Subjective tasks like writing a poem or a joke, where there is no single \"correct\" answer to check against.",
    "explanation": "These domains pose a challenge for standard RL, which requires a clear reward signal.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "verifiable_domains",
    "term": "Verifiable domains",
    "definition": "Tasks like math or coding, where a solution can be automatically and objectively checked for correctness.",
    "explanation": "These are the ideal environments for RL because it's easy to provide the model with a clear success or failure signal.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "zero_shot_learning",
    "term": "Zero-Shot Learning",
    "definition": "When the model does a new task without training or examples.",
    "explanation": "This means you don’t give any examples to the AI. While it’s good for simple tasks, not providing examples might make it harder for the AI to perform well on complex tasks. Giving examples helps, but takes up space in the prompt. You need to balance prompt space with the need for examples.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  }
]