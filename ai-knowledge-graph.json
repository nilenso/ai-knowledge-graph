[
  {
    "id": "agi_artificial_general_intelligence",
    "term": "AGI (Artificial General Intelligence)",
    "definition": "AI that can do any intellectual task a human can.",
    "explanation": "While some define AGI as AI that’s as smart as a human in every way, this isn’t something you need to focus on right now. It’s more important to build AI solutions that solve your specific problems today.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "ai_aritificial_intelligence"
      }
    ]
  },
  {
    "id": "ai_aritificial_intelligence",
    "term": "AI (Aritificial Intelligence)",
    "definition": "A language model like ChatGPT",
    "explanation": "Colloquially we say AI to refer to the current generation LLMs / models. The field of AI is broad and spans decades, covers Machine Learning, Computer Vision, Speech recognition, AGI, Robotics, etc.",
    "category": "Basics",
    "edges": [
      {
        "type": "synonym",
        "target": "model"
      }
    ]
  },
  {
    "id": "model",
    "term": "Model",
    "definition": "Refers to an LLM of a given flavour. Can be base or instruct model.",
    "explanation": "",
    "category": "General",
    "edges": []
  },
  {
    "id": "assistant___instruct_model",
    "term": "Assistant / Instruct model",
    "definition": "A base model that has been fine-tuned to be a honest, harmless, and helpful assistant.",
    "explanation": "This is the type of model most people interact with. The fine-tuning process is what makes it useful for real-world tasks.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "base_model",
    "term": "Base model",
    "definition": "A powerful text-completion engine but not a conversational assistant.",
    "explanation": "There are only 10s of these out there because they're prohibitively expensive to train. They are foundational building blocks that can be customised, and taught new skills or behaviours.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "assistant___instruct_model"
      },
      {
        "type": "related",
        "target": "model"
      }
    ]
  },
  {
    "id": "context",
    "term": "Context",
    "definition": "Same as prompt, but usually refers to the relevant information that's necessary to answer the question or perform the task",
    "explanation": "It is the biggest lever in influencing the AI's response. The context is the model's \"short-term memory\" and it limits the complexity of tasks it can perform.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      },
      {
        "type": "synonym",
        "target": "prompt"
      },
      {
        "type": "related",
        "target": "system_prompt"
      },
      {
        "type": "related",
        "target": "inference"
      }
    ]
  },
  {
    "id": "hallucination",
    "term": "Hallucination",
    "definition": "When AI makes up things that aren’t true.",
    "explanation": "AIs sometimes make stuff up, and you can’t completely stop this. It’s important to be aware that mistakes can happen, so you should check the AI’s answers.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "inference",
    "term": "Inference",
    "definition": "Getting an answer back from the model.",
    "explanation": "When you ask the AI a question and it gives you an answer, that’s called inference. It’s the process of the AI making predictions or responses. Knowing this helps you understand how the AI works and the time or resources it might need to give answers.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "model"
      }
    ]
  },
  {
    "id": "neural_networks",
    "term": "Neural Networks",
    "definition": "Computational models inspired by the brain's structure",
    "explanation": "They are the core technology that enables AI to learn from text and generate human-like responses.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "prompt",
    "term": "Prompt",
    "definition": "The input or question you give to the AI.",
    "explanation": "Giving clear and detailed prompts helps the AI understand what you want. Just like talking to a person, good communication gets better results.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "stochastic",
    "term": "Stochastic",
    "definition": "AI's Nature of being probabalistic, rolling the dice",
    "explanation": "This is why you can get a different answer every time you ask the same question. It allows for creativity but also unpredictability.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "system_prompt",
    "term": "System Prompt",
    "definition": "A hidden instruction given to the model about its persona, rules, and identity.",
    "explanation": "It's a powerful way to steer the AI's behavior consistently without the user seeing the underlying instructions.",
    "category": "Basics",
    "edges": []
  },
  {
    "id": "tokens",
    "term": "Tokens",
    "definition": "The pieces of text that a model understands, it can be a word, a part of a word, or a character.",
    "explanation": "Token impacts model abilities, especially with tasks like spelling or character manipulation. Also, you pay for AI based on the number of tokens used, so knowing about tokens helps manage costs. The vocabulary of tokens depends on training data, and it impacts performance in multilingual or esoteric domains.",
    "category": "Basics",
    "edges": [
      {
        "type": "related",
        "target": "tokenisation"
      },
      {
        "type": "related",
        "target": "encoding"
      },
      {
        "type": "related",
        "target": "vocabulary"
      }
    ]
  },
  {
    "id": "tokenisation",
    "term": "tokenisation"
  },
  {
    "id": "encoding",
    "term": "Encoding",
    "definition": "The process of converting data from a human-readable format (e.g., text, images, audio) into a numerical representation that a machine learning model can process.",
    "explanation": "AI models operate on numbers, not raw data like pixels or characters. Encoding is the crucial bridge that translates real-world information into a mathematical format the model can process, making it the foundational first step for any AI task.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "tokenisation"
      }
    ]
  },
  {
    "id": "distillation",
    "term": "Distillation",
    "definition": "Making a smaller, faster model from a big one.",
    "explanation": "It lets you use cheaper, faster models with less delay (latency). But the smaller model might not be as accurate or powerful as the big one. So, you trade some performance for speed and cost savings.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "hyperparameters",
    "term": "Hyperparameters",
    "definition": "Settings that affect how the model works.",
    "explanation": "These include the number of layers, vocabulary size, and dimensions of the model. By adjusting these settings, you can make the AI learn better. It often takes trying different options to find what works best.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "knowledge_cut_off",
    "term": "Knowledge cut-off",
    "definition": "The date beyond which a model has no information.",
    "explanation": "This is a fundamental limitation of current LLMs, because they stop learning after training is complete. They don't have real-time knowledge unless they are augmented with tools like web search.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "loss",
    "term": "Loss",
    "definition": "A number that measures how wrong the model's predictions are during training.",
    "explanation": "The goal of training is to minimize the loss. Watching the loss go down is how researchers know the model is learning.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "multimodal",
    "term": "Multimodal",
    "definition": "Models that handle different data types like text and images.",
    "explanation": "People use words, pictures, and sounds. When AI can understand all these, it can help users better. Using multimodal AI makes your tools more powerful.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "neural_network_architecture",
    "term": "Neural network architecture",
    "definition": "Design of the math functions that create the neural network",
    "explanation": "It influences ability to learn, scalability, modalities, and training / inference cost. Breakthroughs here are big. Examples include Transformer, Mixture of Experts (MoE), State Space Models (SSMs), Multimodal / Unified, Matryoshka Representation, etc.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "neural_networks"
      },
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "parameters",
    "term": "Parameters",
    "definition": "The learned \"long-term\" memory, represented as billions of numbers",
    "explanation": "Providing more parameters allows training on more data and creating more capabilities. It's also slower and more expensive to train and run. Conversely, smaller models are cheaper and faster, but lesser capable.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "model_size"
      },
      {
        "type": "related",
        "target": "t"
      }
    ]
  },
  {
    "id": "model_size",
    "term": "model size",
    "definition": "",
    "explanation": "",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "vocabulary"
      }
    ]
  },
  {
    "id": "transformer",
    "term": "Transformer",
    "definition": "The main type of neural network architecture used in AI today",
    "explanation": "It provides the crucial ability for understanding language, letting the model focus on relevant parts of input. It makes language models faster, more accurate and capable of capturing nuanced meaning.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "attention"
      },
      {
        "type": "related",
        "target": "neural_network_architecture"
      }
    ]
  },
  {
    "id": "attention",
    "term": "Attention",
    "definition": "Mechanism that lets the model focus on relevant parts of the input",
    "explanation": "This is a crucial ability for understanding language. It makes language models faster, more accurate and capable of capturing nuanced meaning.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "vocabulary",
    "term": "Vocabulary",
    "definition": "The complete set of unique tokens that a model can recognize and produce.",
    "explanation": "The vocabulary defines the atomic units of language the model works with. Its size is a key design choice in an LLM's architecture.",
    "category": "Pre training",
    "edges": []
  },
  {
    "id": "chain_of_thought",
    "term": "Chain of Thought",
    "definition": "Prompting the model to think and plan before answering.",
    "explanation": "When the model thinks first, it gives better answers but takes longer. This trade-off affects speed and quality.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "fine_tuning",
    "term": "Fine-Tuning",
    "definition": "Adjusting a pre-trained model for a specific job.",
    "explanation": "It helps make the AI better for your needs by teaching it with your data. But it might become less good at general tasks. Fine-tuning works best for specific jobs where you need higher accuracy.",
    "category": "Post training",
    "edges": []
  },
  {
    "id": "tools",
    "term": "Tools",
    "definition": "Giving AI the ability to call external programs, like a web search, a calculator, a database, or a code interpreter.",
    "explanation": "This overcomes the model's inherent limitations (like knowledge cut-offs or poor arithmetic) by allowing it to access real-time data and precise computation. Having tools to access the file system or databases can support the \"skill\" of programming.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "tool_use"
      }
    ]
  },
  {
    "id": "tool_use",
    "term": "Tool use"
  },
  {
    "id": "guardrails",
    "term": "Guardrails",
    "definition": "Safety rules to control model outputs.",
    "explanation": "Guardrails help reduce the chance of the AI giving bad or harmful answers. But they are not perfect. It’s important to use them wisely and not rely on them completely.",
    "category": "Post training",
    "edges": []
  },
  {
    "id": "thinking_model",
    "term": "Thinking model",
    "definition": "A model that has undergone extensive RL, making it adept at complex, multi-step problem solving.",
    "explanation": "These models represent the frontier of AI capabilities, moving from simple Q&A to genuine problem-solving.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "reinforcement_learning_rl",
    "term": "Reinforcement Learning (RL)",
    "definition": "Training by making the AI do exercises and learn from its mistakes",
    "explanation": "RL allows the model to go beyond imitating human experts and discover novel, more effective strategies for solving problems on its own. This is how it acquired difficult skills like solving math problems, or writing code.\n\nIt can get the feedback from humans (RLHF) or from another AI model (RLAIF).",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "rlhf"
      },
      {
        "type": "related",
        "target": "rlaif"
      }
    ]
  },
  {
    "id": "rlhf",
    "term": "Reinforcement Learning from Human Feedback (RLHF)",
    "definition": "A technique that uses human preferences to train a \"reward model\" which then guides the reinforcement learning process.",
    "explanation": "RLHF is the key that unlocks RL for subjective, unverifiable domains like writing quality or helpfulness. It's used to fine-tune most modern chatbots.",
    "category": "General",
    "edges": []
  },
  {
    "id": "rlaif",
    "term": "RLAIF"
  },
  {
    "id": "agents",
    "term": "Agents",
    "definition": "AI systems that can autonomously perform a sequence of tasks over time to achieve a complex goal.",
    "explanation": "Agents represent the shift from single-shot commands to having AI assistants that can manage long-running jobs and projects.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "open_weights_model",
    "term": "Open weights model",
    "definition": "A model whose parameters are publicly released, allowing anyone to use, modify, and build upon it.",
    "explanation": "This fosters innovation and competition, preventing the most powerful AI technology from being controlled by only a handful of large corporations.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "parameters"
      }
    ]
  },
  {
    "id": "context_engineering",
    "term": "Context Engineering",
    "definition": "Context engineering is the art and science of strategically assembling all necessary information—such as task descriptions, few-shot examples, retrieved data (RAG), tools, and conversation history—into an LLM's context window to optimize its performance for the next step.",
    "explanation": "An LLM's performance is critically dependent on the quality and relevance of the information in its context window. Providing too little information leads to poor results, while too much or irrelevant information increases costs and can degrade performance.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "embeddings",
    "term": "Embeddings",
    "definition": "Turning words into numbers that show meaning.",
    "explanation": "Embeddings let you search documents by meaning, not just exact words. This helps you find information even if different words are used, making searches smarter and more accurate.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "vector_database"
      }
    ]
  },
  {
    "id": "few_shot_learning",
    "term": "Few-Shot Learning",
    "definition": "Teaching the model with only a few examples.",
    "explanation": "By giving the model examples, you can guide it to behave the way you want. It’s a simple but powerful way to teach the AI what is good or bad.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "retrieval_augmented_generation_rag",
    "term": "Retrieval Augmented Generation (RAG)",
    "definition": "Providing relevant context to the LLM.",
    "explanation": "A language model needs proper context to answer questions. Like a person, it needs access to information such as data, past conversations, or documents to give a good answer. Collecting and giving this info to the AI before asking helps prevent mistakes or it saying, “I don’t know.”",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "vector_database"
      },
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "temperature",
    "term": "Temperature",
    "definition": "A setting that controls how creative AI responses are.",
    "explanation": "Lets you choose between predictable or more imaginative answers. Adjusting temperature can affect the quality and usefulness of the AI’s responses.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "top_p_sampling",
    "term": "Top-p Sampling",
    "definition": "Choosing the next word from top choices making up a set probability.",
    "explanation": "Balances predictability and creativity in AI responses. The trade-off is between safe answers and more varied ones.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "vector_database",
    "term": "Vector Database",
    "definition": "A special database for storing and searching embeddings.",
    "explanation": "They store embeddings of text, images, and more, so you can search by meaning. This makes finding similar items faster and improves searches and recommendations.",
    "category": "AI Engineering",
    "edges": []
  },
  {
    "id": "t",
    "term": "Pre-Training",
    "definition": "The initial stage of building an LLM, where a neural network learns from a vast dataset of text from the internet.",
    "explanation": "This is where the model gains its foundational knowledge about language, facts, and reasoning. The massive scale of this stage is what gives LLMs their broad capabilities.",
    "category": "Pre training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      }
    ]
  },
  {
    "id": "post_training",
    "term": "Post-training",
    "definition": "The second phase of LLM creation, where a pre-trained base model is adapted to become a helpful, instruction-following assistant.",
    "explanation": "This stage bridges the gap between a raw \"internet simulator\" and a useful tool like ChatGPT, aligning the model with human intent.",
    "category": "Post training",
    "edges": [
      {
        "type": "related",
        "target": "base_model"
      },
      {
        "type": "related",
        "target": "fine_tuning"
      },
      {
        "type": "related",
        "target": "reinforcement_learning_rl"
      }
    ]
  },
  {
    "id": "prompt_injection",
    "term": "Prompt Injection",
    "definition": "A security risk where bad instructions are added to prompts.",
    "explanation": "Users might try to trick the AI into ignoring your rules and doing things you don’t want. Knowing about prompt injection helps you protect your AI system from misuse.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "prompt_templates",
    "term": "Prompt Templates",
    "definition": "Pre-made formats for prompts to keep inputs consistent.",
    "explanation": "They help you communicate with the AI consistently by filling in blanks in a set format. This makes it easier to use the AI in different situations and ensures you get good results.",
    "category": "AI Engineering",
    "edges": [
      {
        "type": "related",
        "target": "context_engineering"
      }
    ]
  },
  {
    "id": "sampling",
    "term": "Sampling",
    "definition": "Settings that control predictability, and creativity.",
    "explanation": "Higher temperature for more creative responses, and lower top-p / top-k for more predictable responses. Generating fewer responses also save compute resources.",
    "category": "General",
    "edges": [
      {
        "type": "related",
        "target": "temperature"
      },
      {
        "type": "related",
        "target": "top_p_sampling"
      }
    ]
  }
]